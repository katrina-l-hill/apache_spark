# Assignment: Exploring the Apache Spark Examples

## Objective

The purpose of this assignment is to gain hands-on experience using Apache Spark for distributed data processing and analysis. This project explores data manipulation using Sparkâ€™s Resilient Distributed Datasets (RDDs) and the DataFrame API.

## Overview

In this project, several examples from the official Apache Spark documentation have been recreated and adapted to demonstrate key functionalities, including:

- Data processing using Spark DataFrames.

- Querying structured data using Spark SQL.

## Requirements

### Prerequisites

- To run this project, you need:

   - Python 3.8 or higher

   - Apache Spark (PySpark) installed via pip.

   - Java JDK 8 or higher (required by Spark).

   - A terminal or IDE like VS Code configured to run Python scripts.

### Installation

1. Clone this repository:
   - git clone `https://github.com/katrina-l-hill/apache_spark.git`

2. Install required packages:
   - pip install pyspark

## How to run
- `python spark_test.py`